{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from main import train_setup\n",
    "import sklearn.metrics as skmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/raid/xiaoyuz1/goemotions/goemotions/data/emotions.txt', 'r') as f:\n",
    "    emotion_list = f.readlines()\n",
    "    emotion_list = [s.strip() for s in emotion_list]\n",
    "\n",
    "emotion_text_to_label = dict(zip(emotion_list, range(len(emotion_list))))\n",
    "label_to_emotion_text = dict(zip(range(len(emotion_list)), emotion_list))\n",
    "\n",
    "ekman_to_go = json.load(open('/raid/xiaoyuz1/goemotions/goemotions/data/ekman_mapping.json'))\n",
    "go_to_ekman = {'neutral' : 'neutral'}\n",
    "for k,v in ekman_to_go.items():\n",
    "    for vi in v:\n",
    "        go_to_ekman[vi] = k\n",
    "ekman_texts = list(ekman_to_go.keys())\n",
    "ekman_to_idx = dict(zip(ekman_texts, range(len(ekman_texts))))\n",
    "ekman_to_idx[\"neutral\"] = len(ekman_texts)\n",
    "\n",
    "\n",
    "sentiment_to_go = json.load(open('/raid/xiaoyuz1/goemotions/goemotions/data/sentiment_mapping.json'))\n",
    "go_to_sentiment = {'neutral' : 'neutral'}\n",
    "for k,v in sentiment_to_go.items():\n",
    "    for vi in v:\n",
    "        go_to_sentiment[vi] = k\n",
    "sentiment_texts = list(sentiment_to_go.keys())\n",
    "sentiment_to_idx = dict(zip(sentiment_texts, range(len(sentiment_texts))))\n",
    "sentiment_to_idx[\"neutral\"] = len(sentiment_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"/raid/xiaoyuz1/goemotions/goemotions/data/test.csv\")\n",
    "gt = np.zeros((len(df_test), 28)).astype(int)\n",
    "for i in range(len(df_test)):\n",
    "    row = df_test.iloc[i]\n",
    "    ls = [int(l) for l in row['label'].split(\",\")]\n",
    "    for l in ls:\n",
    "        gt[i, l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred(fname):\n",
    "    fh = open(fname, \"r\")\n",
    "    L = fh.readlines()\n",
    "    logits = []\n",
    "    for line in L:\n",
    "        logit = [float(l) for l in line.strip().split(\" \")]\n",
    "        logits.append(logit)\n",
    "    \n",
    "    y_pred = (torch.tensor(np.asarray(logits).reshape(-1,28)).sigmoid() > 0.5) * 1\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_emotion_result(gt, pred, emotion_list, label2idx=None, text2idx=None):\n",
    "    precs = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for emotion_label, emotion_text in enumerate(emotion_list):\n",
    "        if label2idx is not None:\n",
    "            emotion_idx = label2idx[emotion_label]\n",
    "        else:\n",
    "            assert text2idx is not None\n",
    "            emotion_idx = text2idx[emotion_text]\n",
    "            \n",
    "        y_true = np.asarray(gt)\n",
    "        y_pred = np.asarray(pred)\n",
    "\n",
    "        y_true = y_true[:,emotion_idx]\n",
    "        y_pred = y_pred[:,emotion_idx]\n",
    "\n",
    "        prec = skmetric.precision_score(y_true, y_pred, average=\"binary\")\n",
    "        recall = skmetric.recall_score(y_true, y_pred, average=\"binary\")\n",
    "        f1 = skmetric.f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "        precs += [prec]\n",
    "        recalls += [recall]\n",
    "        f1s += [f1]\n",
    "\n",
    "    df_table_4 = pd.DataFrame({'Emotion': emotion_list,\n",
    "                       'Precision': precs,\n",
    "                       'Recall': recalls,\n",
    "                      'F1' : f1s})\n",
    "    \n",
    "    return df_table_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = dict(zip(range(28), range(28)))\n",
    "idx2label = dict(zip(range(28), range(28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_emo(fname):\n",
    "    df_table = individual_emotion_result(gt, get_y_pred(fname), emotion_list, label2idx=label2idx)\n",
    "    emo_ekman = []\n",
    "    emo_sentiment = []\n",
    "    for emo in emotion_list:\n",
    "        emo_ekman += [go_to_ekman[emo]]\n",
    "        emo_sentiment += [go_to_sentiment[emo]]\n",
    "\n",
    "    emo_dict = {}\n",
    "    for c in df_table:\n",
    "        emo_dict[c] = list(df_table[c].to_numpy())\n",
    "    emo_dict['Ekman'] = emo_ekman\n",
    "    emo_dict['Sentiment'] = emo_sentiment\n",
    "\n",
    "    df_emo = pd.DataFrame(emo_dict)\n",
    "    return df_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyuz1/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname = \"/raid/xiaoyuz1/goemotions_result/both-merge-epoch-10/p1-i0/eval_logits.txt\"\n",
    "\n",
    "fname_LARGE = \"/raid/xiaoyuz1/goemotions_result/6-separate/p1-i0/eval_logits.txt\"\n",
    "fname_AFS = \\\n",
    "    \"/raid/xiaoyuz1/goemotions_result/goemotion-prompt-ekman_neg-small-batch-epoch-5/p1-i0/eval_logits.txt\"\n",
    "\n",
    "# df_table_LARGE = individual_emotion_result(gt, get_y_pred(fname_LARGE), emotion_list, label2idx=label2idx)\n",
    "# df_table_AFS = individual_emotion_result(gt, get_y_pred(fname_AFS), emotion_list, label2idx=label2idx)\n",
    "\n",
    "df_emo_LARGE = get_df_emo(fname_LARGE)\n",
    "df_emo_AFS = get_df_emo(fname_AFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyuz1/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname_merge_AFS = \"/raid/xiaoyuz1/goemotions_result/both-merge-epoch-10/p1-i0/eval_logits.txt\"\n",
    "fname_merge_LARGE = \"/raid/xiaoyuz1/goemotions_result/both_merge_6-epoch-6/p1-i0/eval_logits.txt\"\n",
    "\n",
    "df_emo_merge_LARGE = get_df_emo(fname_merge_LARGE)\n",
    "df_emo_merge_AFS = get_df_emo(fname_merge_AFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision    0.502704\n",
      "Recall       0.443017\n",
      "F1           0.457436\n",
      "dtype: float64\n",
      "Precision    0.547643\n",
      "Recall       0.454240\n",
      "F1           0.476004\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-437ac9c5c67b>:1: FutureWarning:\n",
      "\n",
      "Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "\n",
      "<ipython-input-10-437ac9c5c67b>:2: FutureWarning:\n",
      "\n",
      "Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_emo_merge_LARGE.mean())\n",
    "print(df_emo_merge_AFS.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_table.sort_values(by=['F1'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_neg(df_emo):\n",
    "    print(df_emo[df_emo['Sentiment'] == 'negative'].mean())\n",
    "    return df_emo[df_emo['Sentiment'] == 'negative'].sort_values(by=[\"F1\"], ascending=False)\n",
    "\n",
    "def show_pos(df_emo):\n",
    "    print(df_emo[df_emo['Sentiment'] == 'positive'].mean())\n",
    "    return df_emo[df_emo['Sentiment'] == 'positive'].sort_values(by=[\"F1\"], ascending=False)\n",
    "\n",
    "\n",
    "# df_emo.sort_values(by=['F1'], ascending=False)\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_neg(df_emo_LARGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_neg(df_emo_AFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_neg(df_emo_merge_AFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_neg(df_emo_merge_LARGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe(gt, pred, row_i, study_emo):\n",
    "    print(df_test.iloc[row_i].text)\n",
    "    gt_idxs = np.where(gt[row_i])[0]\n",
    "    pred_idxs = np.where(pred[row_i])[0]\n",
    "    gts = []\n",
    "    \n",
    "    for i in gt_idxs:\n",
    "        l1 = label_to_emotion_text[idx2label[i]]\n",
    "        gts += [l1]\n",
    "    print(\"gt: \", ' '.join(gts))\n",
    "    preds = []\n",
    "    for i in pred_idxs:\n",
    "        preds += [label_to_emotion_text[idx2label[i]]]\n",
    "    print(\"pred: \", ' '.join(preds))\n",
    "    \n",
    "    preds_filtered = []\n",
    "    if(len(preds) == 0):\n",
    "        return [\"null\"]\n",
    "    for i in preds:\n",
    "        if not(i == study_emo):\n",
    "             preds_filtered.append(i)\n",
    "        \n",
    "    return preds_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe(gt, pred, row_i, study_emo):\n",
    "#     print(df_test.iloc[row_i].text)\n",
    "    gt_idxs = np.where(gt[row_i])[0]\n",
    "    pred_idxs = np.where(pred[row_i])[0]\n",
    "    gts = []\n",
    "    \n",
    "    for i in gt_idxs:\n",
    "        l1 = label_to_emotion_text[idx2label[i]]\n",
    "        gts += [l1]\n",
    "#     print(\"gt: \", ' '.join(gts))\n",
    "    preds = []\n",
    "    for i in pred_idxs:\n",
    "        preds += [label_to_emotion_text[idx2label[i]]]\n",
    "    \n",
    "    preds_filtered = []\n",
    "    all_preds = []\n",
    "    if(len(preds) == 0):\n",
    "        return [\"null\"],[\"null\"]\n",
    "    for i in preds:\n",
    "        all_preds.append(i)\n",
    "        if not(i == study_emo):\n",
    "             preds_filtered.append(i)\n",
    "        \n",
    "    return preds_filtered,preds\n",
    "\n",
    "def print_individual(fname, df_emo):\n",
    "    y_pred = get_y_pred(fname).numpy()\n",
    "    #for study_emo in list(df_emo[df_emo['Sentiment'] == 'negative']['Emotion']):\n",
    "    for study_emo in [\"fear\", \"remorse\", \"sadness\", \"anger\", \"disgust\", \"embarrassment\", \"disapproval\", \"nervousness\", \"annoyance\", \"grief\", \"disappointment\"]:\n",
    "        study_idx = label2idx[emotion_text_to_label[study_emo]]\n",
    "        mask = gt[:, study_idx] > 0\n",
    "        got_wrong_mask = (gt[:, study_idx] != y_pred[:, study_idx]) * mask\n",
    "        wrong_row_idxs = np.where(got_wrong_mask)[0]  \n",
    "        preds = []\n",
    "        all_preds = []\n",
    "        for row_idx in wrong_row_idxs:\n",
    "            preds_i,all_preds_i = probe(gt, y_pred, row_idx, study_emo)\n",
    "            preds += preds_i\n",
    "            all_preds += all_preds_i\n",
    "\n",
    "        wrong_labels, wrong_counts = np.unique(preds, return_counts=True)\n",
    "\n",
    "        wrong_labels2 = []\n",
    "        wrong_counts2 = []\n",
    "        for k in np.argsort(wrong_counts)[::-1]:\n",
    "            wrong_labels2 += [wrong_labels[k]]\n",
    "            wrong_counts2 += [wrong_counts[k]]\n",
    "        #print(np.sum(wrong_counts2), np.sum(mask))\n",
    "        #print(\"{}: \".format(study_emo), list(zip(wrong_labels2[:5], np.asarray(wrong_counts2[:5]) / np.sum(mask) ))) \n",
    "        #print(\"\\n\")# \n",
    "        print(\"{} & {} ({:.2f})\".format(study_emo, wrong_labels2[0], wrong_counts2[0] / np.sum(mask) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear & null (0.10)\n",
      "remorse & sadness (0.05)\n",
      "sadness & null (0.10)\n",
      "anger & annoyance (0.18)\n",
      "disgust & annoyance (0.13)\n",
      "embarrassment & null (0.16)\n",
      "disapproval & neutral (0.26)\n",
      "nervousness & fear (0.30)\n",
      "annoyance & neutral (0.21)\n",
      "grief & sadness (0.33)\n",
      "disappointment & neutral (0.15)\n"
     ]
    }
   ],
   "source": [
    "print_individual(fname_merge_LARGE, df_emo_merge_LARGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear & neutral (0.15)\n",
      "remorse & sadness (0.14)\n",
      "sadness & neutral (0.09)\n",
      "anger & annoyance (0.21)\n",
      "disgust & annoyance (0.18)\n",
      "embarrassment & neutral (0.19)\n",
      "disapproval & neutral (0.28)\n",
      "nervousness & fear (0.17)\n",
      "annoyance & neutral (0.25)\n",
      "grief & anger (0.33)\n",
      "disappointment & neutral (0.23)\n"
     ]
    }
   ],
   "source": [
    "print_individual(fname_LARGE, df_emo_LARGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear & null (0.10)\n",
      "remorse & null (0.09)\n",
      "sadness & null (0.14)\n",
      "anger & null (0.16)\n",
      "disgust & null (0.23)\n",
      "embarrassment & null (0.24)\n",
      "disapproval & null (0.30)\n",
      "nervousness & null (0.35)\n",
      "annoyance & neutral (0.22)\n",
      "grief & neutral (0.33)\n",
      "disappointment & null (0.28)\n"
     ]
    }
   ],
   "source": [
    "print_individual(fname_AFS, df_emo_AFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear & neutral (0.13)\n",
      "remorse & sadness (0.12)\n",
      "sadness & remorse (0.08)\n",
      "anger & annoyance (0.22)\n",
      "disgust & annoyance (0.17)\n",
      "embarrassment & null (0.08)\n",
      "disapproval & neutral (0.25)\n",
      "nervousness & fear (0.35)\n",
      "annoyance & neutral (0.21)\n",
      "grief & surprise (0.17)\n",
      "disappointment & neutral (0.17)\n"
     ]
    }
   ],
   "source": [
    "print_individual(fname_merge_AFS, df_emo_merge_AFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study_emo = \"anger\"\n",
    "print(\"label: \", emotion_text_to_label[study_emo])\n",
    "study_idx = label2idx[emotion_text_to_label[study_emo]]\n",
    "print(\"idx: \", study_idx)\n",
    "\n",
    "\n",
    "mask = gt[:, study_idx] > 0\n",
    "print(\"total: \", np.sum(mask))\n",
    "\n",
    "\n",
    "got_wrong_mask = (gt[:, study_idx] != y_pred[:, study_idx]) * mask\n",
    "print(\"got wrong total: \", np.sum(got_wrong_mask))\n",
    "print(\"\\n\")\n",
    "wrong_row_idxs = np.where(got_wrong_mask)[0]  \n",
    "\n",
    "preds = []\n",
    "for row_idx in wrong_row_idxs:\n",
    "    print(row_idx)\n",
    "    preds += probe(gt, y_pred, row_idx, study_emo)\n",
    "    print(\"\\n\")\n",
    "\n",
    "wrong_labels, wrong_counts = np.unique(preds, return_counts=True)\n",
    "wrong_labels2 = []\n",
    "wrong_counts2 = []\n",
    "for k in np.argsort(wrong_counts)[::-1]:\n",
    "    wrong_labels2 += [wrong_labels[k]]\n",
    "    wrong_counts2 += [wrong_counts[k]]\n",
    "\n",
    "ss = []\n",
    "for l,c in zip(wrong_labels2, wrong_counts2):\n",
    "    print(l, c, c/np.sum(wrong_counts2))\n",
    "    ss += [\"{} \".format(l)]\n",
    "\n",
    "\n",
    "wrong_freq_row = \"\\textbf{}  & \"\n",
    "wrong_freq_row += \", \".join(ss[:5])\n",
    "wrong_freq_row + \" \\\\ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true_study_idx = np.ones(np.sum(got_wrong_mask)) * study_idx\n",
    "y_pred_study_idx = []\n",
    "for row in y_pred[got_wrong_mask]:\n",
    "    row_wrong_idxs = list(np.where(row)[0])\n",
    "    row_wrongs = []\n",
    "    for j in row_wrong_idxs:\n",
    "        row_wrongs += [idx2label[j] ]\n",
    "    \n",
    "        \n",
    "    y_pred_study_idx += []\n",
    "\n",
    "wrong_labels, wrong_counts = np.unique(y_pred_study_idx, return_counts=True)\n",
    "for k in np.argsort(wrong_counts)[::-1]:\n",
    "#     print(label)\n",
    "    print(label_to_emotion_text[wrong_labels[k]], wrong_counts[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.iloc[1294]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
